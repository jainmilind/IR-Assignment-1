{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index Construction and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import textract\n",
    "from nltk.corpus import *\n",
    "from nltk.stem.porter import *\n",
    "import os\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before constructing the inverting index we need to install essential datasets for stopword removal and normalization done during the preprocessing step. We have used the available nltk datasets for the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Uncomment in first run \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Porter stemming algorithm is used for the purpose of normalization of tokenized words. It reduces a given term to its root form, thus the query is robust to more typical morphological and inflexional endings. We have the used the PorterStemmer by nltk library for the same purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming porter object\n",
    "stemmer = PorterStemmer()\n",
    "root = Path(\".\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before construction of inverted index, we have converted all the pdf files given to word documents as directly reading from pdfs lead to a higher loss as compared to word documents and the pdfs might contain watermarks or unselectable text which can hamper our text extraction purpose. We have used **textract** and **sent_tokenize** by nltk library to extract the raw text from the documents and tokenize every sentence to preserve the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all doc names\n",
    "files = list()\n",
    "for dir in [r\"\\Auto\", r\"\\Property\"]:\n",
    "    cur_dir = r\".\\Docs\" + dir\n",
    "    for file in os.listdir(cur_dir):\n",
    "        cur_path = r\".\\Docs\" + dir + \"\\\\\" + file\n",
    "        files.append(cur_path)\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list()\n",
    "for x in range(len(files)):\n",
    "    for i in sent_tokenize(textract.process(files[x]).decode(\"utf8\")): \n",
    "        docs.append((i, x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index Construction\n",
    "The structure of our inverted index is a list of tuples of form *(doc index, file index, frequency)* where doc index is the index of the newly transformed documents, file index is the index of file the transformed document belongs to, and the frequency is the number of times the normalized term or the **key** appears in the transformed document. The size of our transformed documents is dynamic in nature, the minimum threshold we have kept is 500 characters. We keep on adding sentences to a document until we get atleast 500 characters. Benefit of adding sentences instead of words is that context of each transformed document is preserved, it starts with a sentence and there is no abrupt ending of the context when the user queries for any document.\n",
    "\n",
    "For a given sentence, we tokenize it to words using **nltk.word_tokenize** and convert those words to lower case. if the word is a stopword, we skip it, otherwise we normalize the word using porter stemming and construct the inverted index with **key** as the normalized term. Our inverted_index is a dictionary whose **key** is the normalized term and **value** is the list of tuples as mentioned above.\n",
    "\n",
    "After the construction of the inverted index table, we sort each list by the third element of the tuple, that is frequency of the **key** so that we get relevant results whenever user queries for anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\"\"\"\n",
    "{\n",
    "   key : string (normalized)\n",
    "   value: list of (\"doc index\", file_index, frequency) \n",
    "}\n",
    "\"\"\"\n",
    "inverted_idx = dict()\n",
    "\n",
    "# list of string modified document\n",
    "documents = list()\n",
    "\n",
    "count_id = 0\n",
    "\n",
    "def process(doc_index):\n",
    "    \"\"\"\n",
    "    Reads file, tokenize it, normalizes it and builds the inverted index\n",
    "    \"\"\"\n",
    "\n",
    "    result = doc_index + 1\n",
    "    global count_id\n",
    "    text = docs[doc_index][0]\n",
    "    file_index = docs[doc_index][1]\n",
    "    while doc_index + 1 < len(docs):\n",
    "        doc_index += 1\n",
    "        if docs[doc_index][1] == docs[doc_index - 1][1] and len(text + docs[doc_index][0]) <= 500:\n",
    "            text += docs[doc_index][0]\n",
    "        else:\n",
    "            result = doc_index\n",
    "            break\n",
    "\n",
    "    tokens = nltk.tokenize.word_tokenize(str(text))\n",
    "\n",
    "    new_token = list()\n",
    "    for i in tokens:\n",
    "        new_token.append(i.lower())\n",
    "    tokens = new_token\n",
    "\n",
    "    curr_str = \"\"\n",
    "    normalised_word_freq = dict()\n",
    "    for j in range(len(tokens)):\n",
    "        curr_str += tokens[j] + \" \"\n",
    "\n",
    "        normal = stemmer.stem(tokens[j].lower())\n",
    "        if normalised_word_freq.get(normal) != None:\n",
    "            normalised_word_freq[normal] += 1\n",
    "        else:\n",
    "            normalised_word_freq[normal] = 1 \n",
    "\n",
    "    documents.append((curr_str, files[file_index]))\n",
    "    \n",
    "    visited = set()\n",
    "    for j in range(len(tokens)):\n",
    "        normalised_word = stemmer.stem(tokens[j].lower())\n",
    "        if tokens[j].lower() not in stop_words and normalised_word not in visited:\n",
    "            visited.add(normalised_word)\n",
    "\n",
    "            if inverted_idx.get(normalised_word) != None:\n",
    "                inverted_idx[normalised_word].append((count_id, file_index, normalised_word_freq[normalised_word]))\n",
    "            else:\n",
    "                inverted_idx[normalised_word] = [(count_id, file_index, normalised_word_freq[normalised_word])]\n",
    "    count_id += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "i = 0\n",
    "while i < len(docs):\n",
    "    i = process(i)\n",
    "\n",
    "for x in inverted_idx:\n",
    "    inverted_idx[x] = sorted(inverted_idx[x], key=lambda y: -y[2])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the entire preprocessing stage inverted index table, transformed document list and the list of original files is dumped to respective binary files so that whenever user wants to query, our search engine can just open those files and use those data structures for querying directly instead of preprocessing again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = root / \"Pickled_files\" / \"Inverted_index\"\n",
    "dbfile = open(my_path, 'wb')\n",
    "pickle.dump(inverted_idx, dbfile) \n",
    "dbfile.close()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Documents\"\n",
    "dbfile = open(my_path, 'wb')\n",
    "pickle.dump(documents, dbfile) \n",
    "dbfile.close()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Files\"\n",
    "dbfile = open(my_path, 'wb')\n",
    "pickle.dump(files, dbfile) \n",
    "dbfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
