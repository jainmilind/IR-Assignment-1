{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import textract\n",
    "from nltk.corpus import *\n",
    "from nltk.stem.porter import *\n",
    "import os\n",
    "from Constants import *\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Uncomment in first run \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming porter object\n",
    "stemmer = PorterStemmer()\n",
    "root = Path(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all doc names\n",
    "files = list()\n",
    "for dir in [r\"\\Auto\", r\"\\Property\"]:\n",
    "    cur_dir = r\".\\Docs\" + dir\n",
    "    for file in os.listdir(cur_dir):\n",
    "        cur_path = r\".\\Docs\" + dir + \"\\\\\" + file\n",
    "        files.append(cur_path)\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list()\n",
    "for x in range(len(files)):\n",
    "    for i in sent_tokenize(textract.process(files[x]).decode(\"utf8\")): \n",
    "        docs.append((i, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\"\"\"\n",
    "{\n",
    "   key : string (normalized)\n",
    "   value: list of (\"doc index\", file_index, frequency) \n",
    "}\n",
    "\"\"\"\n",
    "inverted_idx = dict()\n",
    "\n",
    "# list of string modified document\n",
    "documents = list()\n",
    "\n",
    "count_id = 0\n",
    "\n",
    "def process(doc_index):\n",
    "    \"\"\"\n",
    "    Reads file, tokenize it, normalizes it and builds the inverted index\n",
    "    \"\"\"\n",
    "\n",
    "    result = 0\n",
    "    global count_id\n",
    "    text = docs[doc_index][0]\n",
    "    file_index = docs[doc_index][1]\n",
    "    if doc_index + 1 < len(docs):\n",
    "        if docs[doc_index + 1][1] == docs[doc_index][1]:\n",
    "            text += docs[doc_index + 1][0]\n",
    "            if doc_index + 2 < len(docs):\n",
    "                if docs[doc_index + 2][1] == docs[doc_index][1]:\n",
    "                    text += docs[doc_index + 2][0]\n",
    "                else:\n",
    "                    result = -1\n",
    "        else:\n",
    "            result = -2\n",
    "\n",
    "    tokens = nltk.tokenize.word_tokenize(str(text))\n",
    "\n",
    "    new_token = list()\n",
    "    for i in tokens:\n",
    "        new_token.append(i.lower())\n",
    "    tokens = new_token\n",
    "\n",
    "    curr_str = \"\"\n",
    "    normalised_word_freq = dict()\n",
    "    for j in range(len(tokens)):\n",
    "        curr_str += tokens[j] + \" \"\n",
    "\n",
    "        normal = stemmer.stem(tokens[j].lower())\n",
    "        if normalised_word_freq.get(normal) != None:\n",
    "            normalised_word_freq[normal] += 1\n",
    "        else:\n",
    "            normalised_word_freq[normal] = 1 \n",
    "\n",
    "    documents.append((curr_str, files[file_index]))\n",
    "    \n",
    "    visited = set()\n",
    "    for j in range(len(tokens)):\n",
    "        normalised_word = stemmer.stem(tokens[j].lower())\n",
    "        if tokens[j].lower() not in stop_words and normalised_word not in visited:\n",
    "            visited.add(normalised_word)\n",
    "\n",
    "            if inverted_idx.get(normalised_word) != None:\n",
    "                inverted_idx[normalised_word].append((count_id, file_index, normalised_word_freq[normalised_word]))\n",
    "            else:\n",
    "                inverted_idx[normalised_word] = [(count_id, file_index, normalised_word_freq[normalised_word])]\n",
    "    count_id += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "i = 0\n",
    "while i < len(docs):\n",
    "    x = process(i)\n",
    "    i += 3\n",
    "    i += x\n",
    "\n",
    "for x in inverted_idx:\n",
    "    inverted_idx[x] = sorted(inverted_idx[x], key=lambda y: -y[2])\n",
    "\n",
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = root / \"Pickled_files\" / \"Inverted_index\"\n",
    "dbfile = open(my_path, 'wb')\n",
    "pickle.dump(inverted_idx, dbfile) \n",
    "dbfile.close()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Documents\"\n",
    "dbfile = open(my_path, 'wb')\n",
    "pickle.dump(documents, dbfile) \n",
    "dbfile.close()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Files\"\n",
    "dbfile = open(my_path, 'wb')\n",
    "pickle.dump(files, dbfile) \n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = root / \"Pickled_files\" / \"Inverted_index\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "inverted_idx = pickle.load(dbfile)\n",
    "dbfile.close()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Documents\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "documents = pickle.load(dbfile)\n",
    "dbfile.close()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Files\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "files = pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('deductible amounts 10.1 despite anything in this contract , the insurer shall be liable only for amounts in excess of the applicable deductible amount , if any , mentioned in this contract ; and any provision in this contract relating to an obligation of the insurer to pay an amount or to repair , rebuild or replace property that is damaged or lost shall be satisfied by paying the amount determined by deducting any applicable deductible amount from , the amount the insured would otherwise be entitled to recover , or the cost of repairing , rebuilding or replacing the property.deemed deductible amount ( 2 ) for the purposes of sub condition ( 1 ) , an amount that an insurer is not liable to pay by reason of subsection 261 ( 1 ) or ( 1.1 ) or 263 ( 5.1 ) or ( 5.2.1 ) of the insurance act shall be deemed to be a deductible amount under this contract.termination 11 . ', '.\\\\Docs\\\\Auto\\\\1215E.2.docx'), '.\\\\Docs\\\\Auto\\\\1215E.2.docx'), (('deductible we will pay only when a loss covered under section i or an expense covered under other coverages - section i exceeds the deductible shown in the declarations , and then we will pay only the amount exceeding the deductible.the deductible shall apply to all losses except losses paid under coverage d â€“ loss of use and the following provisions of other coverages - section i : fire department service charge 5.locks refrigerated food spoilage credit card , fund transfer card , forgery and counterfeit money with respect to any one loss , if two or more deductibles under this policy apply to the loss , only the highest deductible amount will apply . ', '.\\\\Docs\\\\Property\\\\5105072011_booklet.docx'), '.\\\\Docs\\\\Property\\\\5105072011_booklet.docx'), (('the value of the loss or damage is based on actual cash value after taking into account depreciation.we will not pay more to repair the automobile than its actual cash value at the time it was damaged , less the deductible specified in your certificate of automobile insurance.we will pay the lower of the following : the cost to repair the loss or damage , less the deductible ; or the actual cash value of the automobile at the time it was damaged , less the applicable deductible . ', '.\\\\Docs\\\\Auto\\\\1215E.2.docx'), '.\\\\Docs\\\\Auto\\\\1215E.2.docx')]\n"
     ]
    }
   ],
   "source": [
    "def query(query_str):\n",
    "    \"\"\"\n",
    "    Normalize query string and search in inverted index and retervive doc\n",
    "    \"\"\"\n",
    "\n",
    "    query_str = stemmer.stem(query_str.lower())\n",
    "    if inverted_idx.get(query_str) == None:\n",
    "        return \"Not found kill yourself\"\n",
    "    else:\n",
    "        ans = []\n",
    "        for i in range(min(len(inverted_idx[query_str]), 3)):\n",
    "            ans.append((documents[inverted_idx[query_str][i][0]], files[inverted_idx[query_str][i][1]]))\n",
    "        return ans\n",
    "\n",
    "print(query(\"deductible\"))\n",
    "# for i in [\"ontario\", \"Milind\", \"illegal\", \"Canadian\", \"dollar\", \"TerrIbLe\", \"PoliCy\", \"Tyre\", \"Mobile\", \"Motor\", \"LMAO\", \"Induction\", \"Proof\"]:\n",
    "    # print(i, query(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1fd8c99ec767a3093790aadbd23b282cb2563d0e033731624352a553ec955a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
