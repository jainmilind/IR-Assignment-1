{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99b9875",
   "metadata": {},
   "source": [
    "# Ranking documents using skip grams word embedding model for phrase queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d31553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import *\n",
    "from nltk.stem.porter import *\n",
    "import os\n",
    "from Constants import *\n",
    "import pickle\n",
    "import gensim \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338ec254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Uncomment in first run \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836cd19",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### The corpus/documents are extracted from the pickle files.\n",
    "#### Stemmer has been initialised to convert term to its root form (Example : received -> receive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923b920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "root = Path(\".\")\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Documents\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "documents = pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52228594",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Skip-gram model is a type of neural network architecture that is commonly used for word embedding. It is a form of unsupervised learning that aims to learn the distributional representation of words.The basic idea behind skip-gram model is to predict the context words surrounding a given target word, rather than predicting the target word from the context words. The context words are defined as the words that occur within a certain window of the target word.\n",
    "\n",
    "The skip-gram model consists of an input layer, a hidden layer, and an output layer. The input layer represents the target word, and the output layer represents the context words. The hidden layer is used to transform the input word into a distributed representation, or embedding, which is used to predict the context words.During training, the skip-gram model is fed with a large corpus of text. For each target word in the corpus, a training example is created by randomly selecting one of its context words as the output, and setting the remaining context words as inputs to the model. The model is then trained to predict the output context word given the input context words.\n",
    "\n",
    "The training process involves adjusting the weights of the model to minimize the difference between the predicted context word and the actual context word. The weights are adjusted using backpropagation algorithm, which updates the weights based on the difference between the predicted and actual output.After training, the skip-gram model generates a dense vector representation for each word in the vocabulary. The vector representation captures the semantic and syntactic information of the word, and can be used as input to other machine learning models for various natural language processing tasks such as text classification, information retrieval, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81911a93",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "The following code initializes an empty list called data, which will be used to store the tokenized and preprocessed documents. Then, it loops over each document in the documents list and tokenizes each sentence into a list of words. Each word is then converted to lowercase and added to a temporary list called temp. The temp list is then appended to the data list, which now contains a list of tokenized and preprocessed documents.\n",
    "\n",
    "Next, the Word2Vec model from the gensim library is used to train a word embedding model on the data list. The min_count parameter specifies the minimum frequency count of a word in the corpus for it to be included in the model. The vector_size parameter specifies the dimensionality of the word vectors. The window parameter specifies the maximum distance between the target word and the context word within a sentence. The sg parameter specifies the training algorithm to be used - 1 for skip-gram and 0 for CBOW. In this case, sg is set to 1 which corresponds to the skip-gram algorithm for training.\n",
    "\n",
    "After training, the word embedding model skipgram_model contains dense vector representations for each word in the corpus. These vector representations can be used for various natural language processing tasks such as text classification, information retrieval, and machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9757e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "for i in documents: \n",
    "    temp = list()\n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i[0]): \n",
    "        temp.append(j.lower()) \n",
    "    data.append(temp)\n",
    "\n",
    "skipgram_model = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5,sg = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad520f34",
   "metadata": {},
   "source": [
    "## Generating word embeddings for documents\n",
    "\n",
    "The following code defines two functions.\n",
    "\n",
    "1.**preprocess(text)** takes a string of text as input, tokenizes it, removes stop words and filters out words with length less than two. It returns a list of preprocessed tokens.\n",
    "\n",
    "2.**text_embedding(text, model)** takes a string of text and a pre-trained word embedding model as inputs. It preprocesses the text using the preprocess() function, obtains the word embeddings for each word in the preprocessed text from the pre-trained word embedding model, takes the average of the word embeddings to generate a document embedding, and returns the document embedding as a numpy array.\n",
    "\n",
    "Finally, the **document_vecs** variable is assigned a list of document embeddings generated using the text_embedding() function and the pre-trained word embedding model (skipgram_model) for each document in the documents list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed556e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = []\n",
    "    for word in word_tokenize(text, language='english'):\n",
    "        if len(word) >= 2 and word not in stop_words:\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# Define a function to generate text embeddings\n",
    "def text_embedding(text, model):\n",
    "    # Preprocess the text (tokenize, remove stop words, stem)\n",
    "    preprocessed_text = preprocess(text)\n",
    "    # Get the word embeddings for each word in the document\n",
    "    word_embeddings = [model.wv[word] for word in preprocessed_text if word in model.wv.key_to_index]\n",
    "    # Take the average of the word embeddings to generate a document embedding\n",
    "    if len(word_embeddings) > 0:\n",
    "        document_embedding = np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        document_embedding = np.zeros(model.vector_size)\n",
    "    return document_embedding\n",
    "\n",
    "document_vecs = [text_embedding(document[0], skipgram_model) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275971a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011627</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.010881</td>\n",
       "      <td>0.305174</td>\n",
       "      <td>0.099559</td>\n",
       "      <td>-0.301031</td>\n",
       "      <td>-0.069827</td>\n",
       "      <td>0.323303</td>\n",
       "      <td>0.146935</td>\n",
       "      <td>-0.055789</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026393</td>\n",
       "      <td>0.053196</td>\n",
       "      <td>0.072542</td>\n",
       "      <td>0.213979</td>\n",
       "      <td>-0.195234</td>\n",
       "      <td>0.186686</td>\n",
       "      <td>-0.005739</td>\n",
       "      <td>0.199159</td>\n",
       "      <td>0.062985</td>\n",
       "      <td>-0.040922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002238</td>\n",
       "      <td>-0.128637</td>\n",
       "      <td>0.078069</td>\n",
       "      <td>0.244151</td>\n",
       "      <td>0.094506</td>\n",
       "      <td>-0.409146</td>\n",
       "      <td>-0.024416</td>\n",
       "      <td>0.433748</td>\n",
       "      <td>0.129218</td>\n",
       "      <td>-0.003537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111575</td>\n",
       "      <td>0.067492</td>\n",
       "      <td>0.209744</td>\n",
       "      <td>0.345303</td>\n",
       "      <td>-0.104026</td>\n",
       "      <td>0.268556</td>\n",
       "      <td>0.013777</td>\n",
       "      <td>0.137338</td>\n",
       "      <td>0.069604</td>\n",
       "      <td>-0.129610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.083339</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>0.181379</td>\n",
       "      <td>0.252945</td>\n",
       "      <td>-0.332915</td>\n",
       "      <td>-0.099393</td>\n",
       "      <td>0.383637</td>\n",
       "      <td>0.089563</td>\n",
       "      <td>0.082441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005498</td>\n",
       "      <td>0.066760</td>\n",
       "      <td>0.139600</td>\n",
       "      <td>0.303438</td>\n",
       "      <td>-0.232925</td>\n",
       "      <td>0.278231</td>\n",
       "      <td>0.172427</td>\n",
       "      <td>0.373989</td>\n",
       "      <td>-0.056544</td>\n",
       "      <td>-0.159889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047034</td>\n",
       "      <td>0.022647</td>\n",
       "      <td>0.036241</td>\n",
       "      <td>0.206683</td>\n",
       "      <td>0.247140</td>\n",
       "      <td>-0.298477</td>\n",
       "      <td>-0.072399</td>\n",
       "      <td>0.305348</td>\n",
       "      <td>0.101278</td>\n",
       "      <td>-0.010541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027765</td>\n",
       "      <td>0.142191</td>\n",
       "      <td>0.191782</td>\n",
       "      <td>0.191793</td>\n",
       "      <td>-0.280872</td>\n",
       "      <td>0.233433</td>\n",
       "      <td>0.200667</td>\n",
       "      <td>0.338396</td>\n",
       "      <td>-0.071003</td>\n",
       "      <td>-0.144382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011981</td>\n",
       "      <td>0.023402</td>\n",
       "      <td>0.040755</td>\n",
       "      <td>0.258638</td>\n",
       "      <td>0.256489</td>\n",
       "      <td>-0.288979</td>\n",
       "      <td>-0.176186</td>\n",
       "      <td>0.269207</td>\n",
       "      <td>0.097166</td>\n",
       "      <td>0.054173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104881</td>\n",
       "      <td>0.059481</td>\n",
       "      <td>0.202893</td>\n",
       "      <td>0.228863</td>\n",
       "      <td>-0.227901</td>\n",
       "      <td>0.228968</td>\n",
       "      <td>0.180517</td>\n",
       "      <td>0.371005</td>\n",
       "      <td>-0.065681</td>\n",
       "      <td>-0.239247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>-0.004867</td>\n",
       "      <td>-0.030920</td>\n",
       "      <td>0.059274</td>\n",
       "      <td>0.093520</td>\n",
       "      <td>0.104142</td>\n",
       "      <td>-0.234052</td>\n",
       "      <td>-0.030891</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.167307</td>\n",
       "      <td>-0.160520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052018</td>\n",
       "      <td>0.071726</td>\n",
       "      <td>0.079872</td>\n",
       "      <td>0.129999</td>\n",
       "      <td>-0.091480</td>\n",
       "      <td>0.375989</td>\n",
       "      <td>0.048597</td>\n",
       "      <td>0.182046</td>\n",
       "      <td>0.038663</td>\n",
       "      <td>-0.091841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>-0.081024</td>\n",
       "      <td>0.027247</td>\n",
       "      <td>0.068879</td>\n",
       "      <td>0.218075</td>\n",
       "      <td>0.149788</td>\n",
       "      <td>-0.190551</td>\n",
       "      <td>-0.061587</td>\n",
       "      <td>0.236786</td>\n",
       "      <td>0.056047</td>\n",
       "      <td>-0.115653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008159</td>\n",
       "      <td>0.038606</td>\n",
       "      <td>0.086236</td>\n",
       "      <td>0.190957</td>\n",
       "      <td>-0.096757</td>\n",
       "      <td>0.225651</td>\n",
       "      <td>0.030694</td>\n",
       "      <td>0.155867</td>\n",
       "      <td>0.097484</td>\n",
       "      <td>-0.064644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>-0.122089</td>\n",
       "      <td>-0.091714</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>0.078946</td>\n",
       "      <td>0.024101</td>\n",
       "      <td>-0.251328</td>\n",
       "      <td>0.048014</td>\n",
       "      <td>0.143938</td>\n",
       "      <td>0.049801</td>\n",
       "      <td>-0.019700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064424</td>\n",
       "      <td>-0.035818</td>\n",
       "      <td>0.101320</td>\n",
       "      <td>0.209992</td>\n",
       "      <td>0.164360</td>\n",
       "      <td>0.355735</td>\n",
       "      <td>0.112376</td>\n",
       "      <td>-0.090869</td>\n",
       "      <td>0.044574</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>-0.121006</td>\n",
       "      <td>0.091159</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.277010</td>\n",
       "      <td>-0.122839</td>\n",
       "      <td>-0.091282</td>\n",
       "      <td>0.028686</td>\n",
       "      <td>0.242289</td>\n",
       "      <td>0.242448</td>\n",
       "      <td>-0.007706</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034037</td>\n",
       "      <td>-0.019213</td>\n",
       "      <td>0.083702</td>\n",
       "      <td>0.086762</td>\n",
       "      <td>0.186181</td>\n",
       "      <td>0.161484</td>\n",
       "      <td>-0.080373</td>\n",
       "      <td>-0.090528</td>\n",
       "      <td>0.332764</td>\n",
       "      <td>0.077187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>-0.049523</td>\n",
       "      <td>0.027049</td>\n",
       "      <td>0.041819</td>\n",
       "      <td>0.037859</td>\n",
       "      <td>-0.008684</td>\n",
       "      <td>-0.034690</td>\n",
       "      <td>-0.020987</td>\n",
       "      <td>0.042684</td>\n",
       "      <td>0.019594</td>\n",
       "      <td>-0.024102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010231</td>\n",
       "      <td>-0.002680</td>\n",
       "      <td>0.052122</td>\n",
       "      <td>0.033962</td>\n",
       "      <td>0.054652</td>\n",
       "      <td>0.069258</td>\n",
       "      <td>0.066837</td>\n",
       "      <td>-0.008567</td>\n",
       "      <td>0.020407</td>\n",
       "      <td>0.033862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6070 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.011627  0.009185  0.010881  0.305174  0.099559 -0.301031 -0.069827   \n",
       "1    -0.002238 -0.128637  0.078069  0.244151  0.094506 -0.409146 -0.024416   \n",
       "2     0.083339  0.001652  0.008078  0.181379  0.252945 -0.332915 -0.099393   \n",
       "3     0.047034  0.022647  0.036241  0.206683  0.247140 -0.298477 -0.072399   \n",
       "4     0.011981  0.023402  0.040755  0.258638  0.256489 -0.288979 -0.176186   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6065 -0.004867 -0.030920  0.059274  0.093520  0.104142 -0.234052 -0.030891   \n",
       "6066 -0.081024  0.027247  0.068879  0.218075  0.149788 -0.190551 -0.061587   \n",
       "6067 -0.122089 -0.091714  0.082900  0.078946  0.024101 -0.251328  0.048014   \n",
       "6068 -0.121006  0.091159  0.004376  0.277010 -0.122839 -0.091282  0.028686   \n",
       "6069 -0.049523  0.027049  0.041819  0.037859 -0.008684 -0.034690 -0.020987   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "0     0.323303  0.146935 -0.055789  ... -0.026393  0.053196  0.072542   \n",
       "1     0.433748  0.129218 -0.003537  ...  0.111575  0.067492  0.209744   \n",
       "2     0.383637  0.089563  0.082441  ...  0.005498  0.066760  0.139600   \n",
       "3     0.305348  0.101278 -0.010541  ...  0.027765  0.142191  0.191782   \n",
       "4     0.269207  0.097166  0.054173  ...  0.104881  0.059481  0.202893   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6065  0.305300  0.167307 -0.160520  ...  0.052018  0.071726  0.079872   \n",
       "6066  0.236786  0.056047 -0.115653  ... -0.008159  0.038606  0.086236   \n",
       "6067  0.143938  0.049801 -0.019700  ...  0.064424 -0.035818  0.101320   \n",
       "6068  0.242289  0.242448 -0.007706  ... -0.034037 -0.019213  0.083702   \n",
       "6069  0.042684  0.019594 -0.024102  ...  0.010231 -0.002680  0.052122   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "0     0.213979 -0.195234  0.186686 -0.005739  0.199159  0.062985 -0.040922  \n",
       "1     0.345303 -0.104026  0.268556  0.013777  0.137338  0.069604 -0.129610  \n",
       "2     0.303438 -0.232925  0.278231  0.172427  0.373989 -0.056544 -0.159889  \n",
       "3     0.191793 -0.280872  0.233433  0.200667  0.338396 -0.071003 -0.144382  \n",
       "4     0.228863 -0.227901  0.228968  0.180517  0.371005 -0.065681 -0.239247  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6065  0.129999 -0.091480  0.375989  0.048597  0.182046  0.038663 -0.091841  \n",
       "6066  0.190957 -0.096757  0.225651  0.030694  0.155867  0.097484 -0.064644  \n",
       "6067  0.209992  0.164360  0.355735  0.112376 -0.090869  0.044574  0.028526  \n",
       "6068  0.086762  0.186181  0.161484 -0.080373 -0.090528  0.332764  0.077187  \n",
       "6069  0.033962  0.054652  0.069258  0.066837 -0.008567  0.020407  0.033862  \n",
       "\n",
       "[6070 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(document_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a78fe8",
   "metadata": {},
   "source": [
    "## Similarity measure\n",
    "\n",
    "Here cosine_similarity is used to compare the query vector and and the document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a0df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a3d47",
   "metadata": {},
   "source": [
    "## Saving document vectors for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04a51f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = root / \"Pickled_files\" / \"Document_vectors\"\n",
    "dbfile = open(my_path, 'wb')\n",
    "pickle.dump(document_vecs, dbfile) \n",
    "dbfile.close()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Document_vectors\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "document_vecs = pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576c5e7",
   "metadata": {},
   "source": [
    "## Processing the query\n",
    "\n",
    "This code calculates the similarity scores between a given query and a set of documents. It first generates a document vector for each document in the set using the text_embedding function. Then, it generates a query vector using the same function. The cosine similarity between the query vector and each document vector is calculated and stored in the similarity_scores list. Finally, the documents are ranked in descending order based on their similarity scores with the query, and the top five documents are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e2bf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('any person , other than you or a resident relative , while using a non-owned auto : which is available for hire by the public , or separate premium is charged for each auto . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\AU127-1.docx'),\n",
       " ('an unidentified automobile is one whose owner or driver can not be determined . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\1215E.2.docx'),\n",
       " ('for purposes of this policy , a private passenger type auto shall be deemed to be owned by a any of the following types of vehicles on the date you become the owner : person if leased : a. a private passenger auto ; or under a written agreement to that person ; b. a pickup or van . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\PL-600003-87.docx'),\n",
       " ('your driver ’ s license or auto registration has been under suspension or revocation during the policy period . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\7thEditionPolicy.docx'),\n",
       " ('m enot owned by you or a relative who resides in your household , being operated by you with the owner ’ s permission . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\AU127-1.docx')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage: compare a query with a set of documents\n",
    "query = \"Excluded driver\"\n",
    "\n",
    "query_vec = text_embedding(query,skipgram_model)\n",
    "# Calculate similarity scores between the query and all documents\n",
    "similarity_scores = [cosine_similarity(query_vec, document_vec) for document_vec in document_vecs]\n",
    "# Rank documents based on similarity scores\n",
    "ranked_documents = [document for _, document in sorted(zip(similarity_scores, documents), reverse=True)]\n",
    "# Print the ranked documents\n",
    "ranked_documents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90873a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1fd8c99ec767a3093790aadbd23b282cb2563d0e033731624352a553ec955a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
