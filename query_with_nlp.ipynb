{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99b9875",
   "metadata": {},
   "source": [
    "# Ranking documents using skip grams word embedding model for phrase queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18d31553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import *\n",
    "from nltk.stem.porter import *\n",
    "import os\n",
    "from Constants import *\n",
    "import pickle\n",
    "import gensim \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "338ec254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Uncomment in first run \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836cd19",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### The corpus/documents are extracted from the pickle files.\n",
    "#### Stemmer has been initialised to convert term to its root form (Example : received -> receive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "923b920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "root = Path(\".\")\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Documents\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "documents = pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52228594",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Skip-gram model is a type of neural network architecture that is commonly used for word embedding. It is a form of unsupervised learning that aims to learn the distributional representation of words.The basic idea behind skip-gram model is to predict the context words surrounding a given target word, rather than predicting the target word from the context words. The context words are defined as the words that occur within a certain window of the target word.\n",
    "\n",
    "The skip-gram model consists of an input layer, a hidden layer, and an output layer. The input layer represents the target word, and the output layer represents the context words. The hidden layer is used to transform the input word into a distributed representation, or embedding, which is used to predict the context words.During training, the skip-gram model is fed with a large corpus of text. For each target word in the corpus, a training example is created by randomly selecting one of its context words as the output, and setting the remaining context words as inputs to the model. The model is then trained to predict the output context word given the input context words.\n",
    "\n",
    "The training process involves adjusting the weights of the model to minimize the difference between the predicted context word and the actual context word. The weights are adjusted using backpropagation algorithm, which updates the weights based on the difference between the predicted and actual output.After training, the skip-gram model generates a dense vector representation for each word in the vocabulary. The vector representation captures the semantic and syntactic information of the word, and can be used as input to other machine learning models for various natural language processing tasks such as text classification, information retrieval, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81911a93",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "The following code initializes an empty list called data, which will be used to store the tokenized and preprocessed documents. Then, it loops over each document in the documents list and tokenizes each sentence into a list of words. Each word is then converted to lowercase and added to a temporary list called temp. The temp list is then appended to the data list, which now contains a list of tokenized and preprocessed documents.\n",
    "\n",
    "Next, the Word2Vec model from the gensim library is used to train a word embedding model on the data list. The min_count parameter specifies the minimum frequency count of a word in the corpus for it to be included in the model. The vector_size parameter specifies the dimensionality of the word vectors. The window parameter specifies the maximum distance between the target word and the context word within a sentence. The sg parameter specifies the training algorithm to be used - 1 for skip-gram and 0 for CBOW. In this case, sg is set to 1 which corresponds to the skip-gram algorithm for training.\n",
    "\n",
    "After training, the word embedding model skipgram_model contains dense vector representations for each word in the corpus. These vector representations can be used for various natural language processing tasks such as text classification, information retrieval, and machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9757e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "for i in documents: \n",
    "    temp = list()\n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i[0]): \n",
    "        temp.append(j.lower()) \n",
    "    data.append(temp)\n",
    "\n",
    "skipgram_model = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5,sg = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad520f34",
   "metadata": {},
   "source": [
    "## Generating word embeddings for documents\n",
    "\n",
    "The following code defines two functions.\n",
    "\n",
    "1.**preprocess(text)** takes a string of text as input, tokenizes it, removes stop words and filters out words with length less than two. It returns a list of preprocessed tokens.\n",
    "\n",
    "2.**text_embedding(text, model)** takes a string of text and a pre-trained word embedding model as inputs. It preprocesses the text using the preprocess() function, obtains the word embeddings for each word in the preprocessed text from the pre-trained word embedding model, takes the average of the word embeddings to generate a document embedding, and returns the document embedding as a numpy array.\n",
    "\n",
    "Finally, the **document_vecs** variable is assigned a list of document embeddings generated using the text_embedding() function and the pre-trained word embedding model (skipgram_model) for each document in the documents list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7ed556e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = []\n",
    "    for word in word_tokenize(text, language='english'):\n",
    "        if len(word) >= 2 and word not in stop_words:\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# Define a function to generate text embeddings\n",
    "def text_embedding(text, model):\n",
    "    # Preprocess the text (tokenize, remove stop words, stem)\n",
    "    preprocessed_text = preprocess(text)\n",
    "    # Get the word embeddings for each word in the document\n",
    "    word_embeddings = [model.wv[word] for word in preprocessed_text if word in model.wv.key_to_index]\n",
    "    # Take the average of the word embeddings to generate a document embedding\n",
    "    if len(word_embeddings) > 0:\n",
    "        document_embedding = np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        document_embedding = np.zeros(model.vector_size)\n",
    "    return document_embedding\n",
    "\n",
    "document_vecs = [text_embedding(document[0], skipgram_model) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "275971a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.206675</td>\n",
       "      <td>0.054547</td>\n",
       "      <td>-0.014692</td>\n",
       "      <td>-0.003927</td>\n",
       "      <td>-0.026756</td>\n",
       "      <td>-0.420749</td>\n",
       "      <td>0.283177</td>\n",
       "      <td>0.084087</td>\n",
       "      <td>0.136020</td>\n",
       "      <td>0.021542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373114</td>\n",
       "      <td>-0.125953</td>\n",
       "      <td>0.171524</td>\n",
       "      <td>-0.145394</td>\n",
       "      <td>0.089098</td>\n",
       "      <td>-0.027650</td>\n",
       "      <td>0.143820</td>\n",
       "      <td>-0.260030</td>\n",
       "      <td>0.192953</td>\n",
       "      <td>0.160765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.191518</td>\n",
       "      <td>0.253252</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.008909</td>\n",
       "      <td>-0.018811</td>\n",
       "      <td>-0.477852</td>\n",
       "      <td>0.327341</td>\n",
       "      <td>-0.123260</td>\n",
       "      <td>0.100434</td>\n",
       "      <td>0.023203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354657</td>\n",
       "      <td>-0.081819</td>\n",
       "      <td>0.128444</td>\n",
       "      <td>-0.317003</td>\n",
       "      <td>-0.056611</td>\n",
       "      <td>-0.053263</td>\n",
       "      <td>0.373928</td>\n",
       "      <td>-0.222742</td>\n",
       "      <td>0.236504</td>\n",
       "      <td>-0.034385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.154748</td>\n",
       "      <td>0.157415</td>\n",
       "      <td>-0.026929</td>\n",
       "      <td>-0.196174</td>\n",
       "      <td>-0.063128</td>\n",
       "      <td>-0.440125</td>\n",
       "      <td>0.160929</td>\n",
       "      <td>-0.160880</td>\n",
       "      <td>0.195482</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438138</td>\n",
       "      <td>0.054933</td>\n",
       "      <td>0.130219</td>\n",
       "      <td>-0.266910</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.373477</td>\n",
       "      <td>-0.193283</td>\n",
       "      <td>0.284580</td>\n",
       "      <td>0.119897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.207999</td>\n",
       "      <td>0.176518</td>\n",
       "      <td>0.044923</td>\n",
       "      <td>-0.189760</td>\n",
       "      <td>-0.129191</td>\n",
       "      <td>-0.449643</td>\n",
       "      <td>0.183666</td>\n",
       "      <td>-0.187676</td>\n",
       "      <td>0.187501</td>\n",
       "      <td>0.037843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538991</td>\n",
       "      <td>-0.123464</td>\n",
       "      <td>0.090113</td>\n",
       "      <td>-0.247639</td>\n",
       "      <td>0.078290</td>\n",
       "      <td>0.094601</td>\n",
       "      <td>0.330541</td>\n",
       "      <td>-0.184547</td>\n",
       "      <td>0.155966</td>\n",
       "      <td>0.037984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.174037</td>\n",
       "      <td>0.230450</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>-0.161818</td>\n",
       "      <td>-0.086793</td>\n",
       "      <td>-0.433262</td>\n",
       "      <td>0.255819</td>\n",
       "      <td>-0.219981</td>\n",
       "      <td>0.110463</td>\n",
       "      <td>0.040317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>-0.076896</td>\n",
       "      <td>0.119087</td>\n",
       "      <td>-0.304511</td>\n",
       "      <td>0.050431</td>\n",
       "      <td>0.111769</td>\n",
       "      <td>0.407474</td>\n",
       "      <td>-0.180155</td>\n",
       "      <td>0.247936</td>\n",
       "      <td>0.041905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>0.160291</td>\n",
       "      <td>0.068060</td>\n",
       "      <td>-0.010850</td>\n",
       "      <td>-0.016243</td>\n",
       "      <td>0.042547</td>\n",
       "      <td>-0.301904</td>\n",
       "      <td>0.376517</td>\n",
       "      <td>0.118914</td>\n",
       "      <td>0.029595</td>\n",
       "      <td>0.051652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393338</td>\n",
       "      <td>-0.097152</td>\n",
       "      <td>0.218863</td>\n",
       "      <td>-0.011125</td>\n",
       "      <td>0.152679</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.132301</td>\n",
       "      <td>-0.178840</td>\n",
       "      <td>0.186988</td>\n",
       "      <td>0.025415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>0.116637</td>\n",
       "      <td>0.034341</td>\n",
       "      <td>0.036088</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>-0.402104</td>\n",
       "      <td>0.341002</td>\n",
       "      <td>0.067454</td>\n",
       "      <td>0.171476</td>\n",
       "      <td>-0.002749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391155</td>\n",
       "      <td>-0.119265</td>\n",
       "      <td>0.182735</td>\n",
       "      <td>-0.147085</td>\n",
       "      <td>0.163016</td>\n",
       "      <td>0.110871</td>\n",
       "      <td>0.091892</td>\n",
       "      <td>-0.197560</td>\n",
       "      <td>0.121246</td>\n",
       "      <td>0.063576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>0.122492</td>\n",
       "      <td>0.041628</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.011336</td>\n",
       "      <td>0.059729</td>\n",
       "      <td>-0.387400</td>\n",
       "      <td>0.337439</td>\n",
       "      <td>0.017183</td>\n",
       "      <td>0.117517</td>\n",
       "      <td>-0.011920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391228</td>\n",
       "      <td>-0.072704</td>\n",
       "      <td>0.257140</td>\n",
       "      <td>-0.131345</td>\n",
       "      <td>0.232458</td>\n",
       "      <td>0.077982</td>\n",
       "      <td>0.127659</td>\n",
       "      <td>-0.233226</td>\n",
       "      <td>0.170853</td>\n",
       "      <td>0.066398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>-0.005365</td>\n",
       "      <td>0.023678</td>\n",
       "      <td>0.085230</td>\n",
       "      <td>0.090532</td>\n",
       "      <td>0.042343</td>\n",
       "      <td>-0.291634</td>\n",
       "      <td>0.256536</td>\n",
       "      <td>0.096108</td>\n",
       "      <td>0.041206</td>\n",
       "      <td>0.034731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183279</td>\n",
       "      <td>-0.113884</td>\n",
       "      <td>0.063614</td>\n",
       "      <td>-0.062915</td>\n",
       "      <td>0.123683</td>\n",
       "      <td>0.187335</td>\n",
       "      <td>0.072833</td>\n",
       "      <td>-0.209256</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>-0.006037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>0.132930</td>\n",
       "      <td>-0.351242</td>\n",
       "      <td>0.092209</td>\n",
       "      <td>0.131784</td>\n",
       "      <td>0.075273</td>\n",
       "      <td>-0.101920</td>\n",
       "      <td>0.298432</td>\n",
       "      <td>0.471381</td>\n",
       "      <td>0.065388</td>\n",
       "      <td>0.046039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400269</td>\n",
       "      <td>-0.302683</td>\n",
       "      <td>0.363408</td>\n",
       "      <td>0.079542</td>\n",
       "      <td>0.476036</td>\n",
       "      <td>0.247744</td>\n",
       "      <td>-0.066857</td>\n",
       "      <td>-0.210577</td>\n",
       "      <td>0.064994</td>\n",
       "      <td>-0.014745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2378 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.206675  0.054547 -0.014692 -0.003927 -0.026756 -0.420749  0.283177   \n",
       "1     0.191518  0.253252 -0.036307 -0.008909 -0.018811 -0.477852  0.327341   \n",
       "2     0.154748  0.157415 -0.026929 -0.196174 -0.063128 -0.440125  0.160929   \n",
       "3     0.207999  0.176518  0.044923 -0.189760 -0.129191 -0.449643  0.183666   \n",
       "4     0.174037  0.230450  0.001026 -0.161818 -0.086793 -0.433262  0.255819   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2373  0.160291  0.068060 -0.010850 -0.016243  0.042547 -0.301904  0.376517   \n",
       "2374  0.116637  0.034341  0.036088  0.019712  0.009077 -0.402104  0.341002   \n",
       "2375  0.122492  0.041628 -0.000005 -0.011336  0.059729 -0.387400  0.337439   \n",
       "2376 -0.005365  0.023678  0.085230  0.090532  0.042343 -0.291634  0.256536   \n",
       "2377  0.132930 -0.351242  0.092209  0.131784  0.075273 -0.101920  0.298432   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "0     0.084087  0.136020  0.021542  ...  0.373114 -0.125953  0.171524   \n",
       "1    -0.123260  0.100434  0.023203  ...  0.354657 -0.081819  0.128444   \n",
       "2    -0.160880  0.195482  0.009681  ...  0.438138  0.054933  0.130219   \n",
       "3    -0.187676  0.187501  0.037843  ...  0.538991 -0.123464  0.090113   \n",
       "4    -0.219981  0.110463  0.040317  ...  0.356322 -0.076896  0.119087   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2373  0.118914  0.029595  0.051652  ...  0.393338 -0.097152  0.218863   \n",
       "2374  0.067454  0.171476 -0.002749  ...  0.391155 -0.119265  0.182735   \n",
       "2375  0.017183  0.117517 -0.011920  ...  0.391228 -0.072704  0.257140   \n",
       "2376  0.096108  0.041206  0.034731  ...  0.183279 -0.113884  0.063614   \n",
       "2377  0.471381  0.065388  0.046039  ...  0.400269 -0.302683  0.363408   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "0    -0.145394  0.089098 -0.027650  0.143820 -0.260030  0.192953  0.160765  \n",
       "1    -0.317003 -0.056611 -0.053263  0.373928 -0.222742  0.236504 -0.034385  \n",
       "2    -0.266910  0.009393  0.017474  0.373477 -0.193283  0.284580  0.119897  \n",
       "3    -0.247639  0.078290  0.094601  0.330541 -0.184547  0.155966  0.037984  \n",
       "4    -0.304511  0.050431  0.111769  0.407474 -0.180155  0.247936  0.041905  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2373 -0.011125  0.152679  0.001062  0.132301 -0.178840  0.186988  0.025415  \n",
       "2374 -0.147085  0.163016  0.110871  0.091892 -0.197560  0.121246  0.063576  \n",
       "2375 -0.131345  0.232458  0.077982  0.127659 -0.233226  0.170853  0.066398  \n",
       "2376 -0.062915  0.123683  0.187335  0.072833 -0.209256  0.011695 -0.006037  \n",
       "2377  0.079542  0.476036  0.247744 -0.066857 -0.210577  0.064994 -0.014745  \n",
       "\n",
       "[2378 rows x 100 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(document_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a78fe8",
   "metadata": {},
   "source": [
    "## Similarity measure\n",
    "\n",
    "Here cosine_similarity is used to compare the query vector and and the document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "97a0df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a3d47",
   "metadata": {},
   "source": [
    "## Saving document vectors for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e04a51f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = root / \"Pickled_files\" / \"Document_vectors\"\n",
    "dbfile = open(my_path, 'wb')\n",
    "pickle.dump(document_vecs, dbfile) \n",
    "dbfile.close()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Document_vectors\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "document_vecs = pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576c5e7",
   "metadata": {},
   "source": [
    "## Processing the query\n",
    "\n",
    "This code calculates the similarity scores between a given query and a set of documents. It first generates a document vector for each document in the set using the text_embedding function. Then, it generates a query vector using the same function. The cosine similarity between the query vector and each document vector is calculated and stored in the similarity_scores list. Finally, the documents are ranked in descending order based on their similarity scores with the query, and the top five documents are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "38e2bf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anyone injured while occupying an auto without a reasonable belief that he or she had the consent of the owner to do so.a household member , other than your spouse , while occupying or struck by an auto owned or regularly used by you or any household member unless a premium for this part is shown for that auto on the coverage selections page.you or your spouse , if a household member , while occupying or struck by an auto owned or regularly used by you or your spouse unless a premium for this part is shown for that auto on the coverage selections page . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\7thEditionPolicy.docx'),\n",
       " ('we will not pay damages to or for any household member who has a massachusetts auto policy of his or her own or who is covered by any massachusetts auto policy of another household member providing uninsured auto insurance with higher limits.anyone else while occupying your auto.we will not pay damages to or for anyone else who has a massachusetts auto policy of his or her own , or who is covered by any massachusetts auto policy of another household member providing uninsured auto insurance . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\7thEditionPolicy.docx'),\n",
       " ('however , this does not include an automobile owned by or registered in the name of the insured person or their spouse.what is an unidentified automobile ? an unidentified automobile is one whose owner or driver can not be determined . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\1215E.2.docx'),\n",
       " ('we will consider the driver of the auto covered under this part to be no more than 50 % at fault if : that auto was legally parked when struck by another auto.that auto was struck in the rear by another auto moving in the same direction.the operator of the other auto was convicted of certain violations listed in massachusetts law or any similar law of another state in which the accident occurs . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\7thEditionPolicy.docx'),\n",
       " ('we will not pay damages to or for any household member who has a massachusetts auto policy of his or her own or who is covered by a massachusetts auto policy of another household member providing underinsured auto insurance with higher limits.anyone else while occupying your auto.we will not pay damages to or for anyone else who has a massachusetts auto policy of his or her own or who is covered by a massachusetts auto policy of another household member providing underinsured auto coverage . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\7thEditionPolicy.docx')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage: compare a query with a set of documents\n",
    "query = \"Excluded driver\"\n",
    "\n",
    "query_vec = text_embedding(query,skipgram_model)\n",
    "# Calculate similarity scores between the query and all documents\n",
    "similarity_scores = [cosine_similarity(query_vec, document_vec) for document_vec in document_vecs]\n",
    "# Rank documents based on similarity scores\n",
    "ranked_documents = [document for _, document in sorted(zip(similarity_scores, documents), reverse=True)]\n",
    "# Print the ranked documents\n",
    "ranked_documents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90873a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1fd8c99ec767a3093790aadbd23b282cb2563d0e033731624352a553ec955a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
