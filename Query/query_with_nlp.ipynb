{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99b9875",
   "metadata": {},
   "source": [
    "# Ranking documents using skip grams word embedding model for phrase queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d31553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import *\n",
    "from nltk.stem.porter import *\n",
    "import pickle\n",
    "import gensim \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "338ec254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Uncomment in first run \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836cd19",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### The corpus/documents are extracted from the pickle files.\n",
    "#### Stemmer has been initialised to convert term to its root form (Example : received -> receive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923b920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "root = Path(\"../\")\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Documents\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "documents = pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52228594",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Skip-gram model is a type of neural network architecture that is commonly used for word embedding. It is a form of unsupervised learning that aims to learn the distributional representation of words.The basic idea behind skip-gram model is to predict the context words surrounding a given target word, rather than predicting the target word from the context words. The context words are defined as the words that occur within a certain window of the target word.\n",
    "\n",
    "The skip-gram model consists of an input layer, a hidden layer, and an output layer. The input layer represents the target word, and the output layer represents the context words. The hidden layer is used to transform the input word into a distributed representation, or embedding, which is used to predict the context words.During training, the skip-gram model is fed with a large corpus of text. For each target word in the corpus, a training example is created by randomly selecting one of its context words as the output, and setting the remaining context words as inputs to the model. The model is then trained to predict the output context word given the input context words.\n",
    "\n",
    "The training process involves adjusting the weights of the model to minimize the difference between the predicted context word and the actual context word. The weights are adjusted using backpropagation algorithm, which updates the weights based on the difference between the predicted and actual output.After training, the skip-gram model generates a dense vector representation for each word in the vocabulary. The vector representation captures the semantic and syntactic information of the word, and can be used as input to other machine learning models for various natural language processing tasks such as text classification, information retrieval, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81911a93",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "The following code initializes an empty list called data, which will be used to store the tokenized and preprocessed documents. Then, it loops over each document in the documents list and tokenizes each sentence into a list of words. Each word is then converted to lowercase and added to a temporary list called temp. The temp list is then appended to the data list, which now contains a list of tokenized and preprocessed documents.\n",
    "\n",
    "Next, the Word2Vec model from the gensim library is used to train a word embedding model on the data list. The min_count parameter specifies the minimum frequency count of a word in the corpus for it to be included in the model. The vector_size parameter specifies the dimensionality of the word vectors. The window parameter specifies the maximum distance between the target word and the context word within a sentence. The sg parameter specifies the training algorithm to be used - 1 for skip-gram and 0 for CBOW. In this case, sg is set to 1 which corresponds to the skip-gram algorithm for training.\n",
    "\n",
    "After training, the word embedding model skipgram_model contains dense vector representations for each word in the corpus. These vector representations can be used for various natural language processing tasks such as text classification, information retrieval, and machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9757e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "for i in documents: \n",
    "    temp = list()\n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i[0]): \n",
    "        temp.append(j.lower()) \n",
    "    data.append(temp)\n",
    "\n",
    "skipgram_model = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5,sg = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad520f34",
   "metadata": {},
   "source": [
    "## Generating word embeddings for documents\n",
    "\n",
    "The following code defines two functions.\n",
    "\n",
    "1.**preprocess(text)** takes a string of text as input, tokenizes it, removes stop words and filters out words with length less than two. It returns a list of preprocessed tokens.\n",
    "\n",
    "2.**text_embedding(text, model)** takes a string of text and a pre-trained word embedding model as inputs. It preprocesses the text using the preprocess() function, obtains the word embeddings for each word in the preprocessed text from the pre-trained word embedding model, takes the average of the word embeddings to generate a document embedding, and returns the document embedding as a numpy array.\n",
    "\n",
    "Finally, the **document_vecs** variable is assigned a list of document embeddings generated using the text_embedding() function and the pre-trained word embedding model (skipgram_model) for each document in the documents list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed556e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = []\n",
    "    for word in word_tokenize(text, language='english'):\n",
    "        if len(word) >= 2 and word not in stop_words:\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# Define a function to generate text embeddings\n",
    "def text_embedding(text, model):\n",
    "    # Preprocess the text (tokenize, remove stop words, stem)\n",
    "    preprocessed_text = preprocess(text)\n",
    "    # Get the word embeddings for each word in the document\n",
    "    word_embeddings = [model.wv[word] for word in preprocessed_text if word in model.wv.key_to_index]\n",
    "    # Take the average of the word embeddings to generate a document embedding\n",
    "    if len(word_embeddings) > 0:\n",
    "        document_embedding = np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        document_embedding = np.zeros(model.vector_size)\n",
    "    return document_embedding\n",
    "\n",
    "document_vecs = [text_embedding(document[0], skipgram_model) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275971a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.031992</td>\n",
       "      <td>0.149353</td>\n",
       "      <td>-0.103647</td>\n",
       "      <td>0.127495</td>\n",
       "      <td>-0.345164</td>\n",
       "      <td>0.255643</td>\n",
       "      <td>0.259811</td>\n",
       "      <td>0.157978</td>\n",
       "      <td>-0.124985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450639</td>\n",
       "      <td>0.028335</td>\n",
       "      <td>0.206324</td>\n",
       "      <td>-0.063071</td>\n",
       "      <td>0.110578</td>\n",
       "      <td>0.085154</td>\n",
       "      <td>0.278110</td>\n",
       "      <td>-0.201369</td>\n",
       "      <td>0.117699</td>\n",
       "      <td>0.060710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.105189</td>\n",
       "      <td>0.132897</td>\n",
       "      <td>0.044019</td>\n",
       "      <td>-0.042159</td>\n",
       "      <td>0.179388</td>\n",
       "      <td>-0.327574</td>\n",
       "      <td>0.260852</td>\n",
       "      <td>0.266546</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>-0.069553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422972</td>\n",
       "      <td>0.107587</td>\n",
       "      <td>0.161347</td>\n",
       "      <td>-0.126538</td>\n",
       "      <td>0.073219</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.380425</td>\n",
       "      <td>0.048626</td>\n",
       "      <td>0.157927</td>\n",
       "      <td>-0.044815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131420</td>\n",
       "      <td>0.056091</td>\n",
       "      <td>0.078574</td>\n",
       "      <td>-0.137063</td>\n",
       "      <td>0.160461</td>\n",
       "      <td>-0.292200</td>\n",
       "      <td>0.185550</td>\n",
       "      <td>0.263968</td>\n",
       "      <td>0.205161</td>\n",
       "      <td>-0.011723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424150</td>\n",
       "      <td>0.119437</td>\n",
       "      <td>0.203855</td>\n",
       "      <td>-0.044585</td>\n",
       "      <td>0.162057</td>\n",
       "      <td>0.188318</td>\n",
       "      <td>0.476668</td>\n",
       "      <td>0.031398</td>\n",
       "      <td>0.156915</td>\n",
       "      <td>0.177543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.133949</td>\n",
       "      <td>0.110150</td>\n",
       "      <td>0.059820</td>\n",
       "      <td>-0.149727</td>\n",
       "      <td>0.158738</td>\n",
       "      <td>-0.300730</td>\n",
       "      <td>0.271797</td>\n",
       "      <td>0.353809</td>\n",
       "      <td>0.208494</td>\n",
       "      <td>-0.062116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501836</td>\n",
       "      <td>0.067628</td>\n",
       "      <td>0.204282</td>\n",
       "      <td>-0.040796</td>\n",
       "      <td>0.039807</td>\n",
       "      <td>0.139566</td>\n",
       "      <td>0.335987</td>\n",
       "      <td>-0.004681</td>\n",
       "      <td>0.147508</td>\n",
       "      <td>0.228402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089234</td>\n",
       "      <td>0.104596</td>\n",
       "      <td>0.083269</td>\n",
       "      <td>0.031202</td>\n",
       "      <td>0.128677</td>\n",
       "      <td>-0.220117</td>\n",
       "      <td>0.220752</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.252232</td>\n",
       "      <td>-0.092232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421696</td>\n",
       "      <td>0.113761</td>\n",
       "      <td>0.252042</td>\n",
       "      <td>-0.097961</td>\n",
       "      <td>0.155823</td>\n",
       "      <td>0.296137</td>\n",
       "      <td>0.377603</td>\n",
       "      <td>-0.072417</td>\n",
       "      <td>0.211872</td>\n",
       "      <td>0.034544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>-0.059292</td>\n",
       "      <td>0.093661</td>\n",
       "      <td>-0.053745</td>\n",
       "      <td>-0.151095</td>\n",
       "      <td>0.096664</td>\n",
       "      <td>-0.395085</td>\n",
       "      <td>0.341605</td>\n",
       "      <td>0.403616</td>\n",
       "      <td>0.341046</td>\n",
       "      <td>-0.082672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528257</td>\n",
       "      <td>0.013550</td>\n",
       "      <td>0.255730</td>\n",
       "      <td>-0.059689</td>\n",
       "      <td>0.189930</td>\n",
       "      <td>0.266602</td>\n",
       "      <td>0.204304</td>\n",
       "      <td>-0.140718</td>\n",
       "      <td>0.181028</td>\n",
       "      <td>0.086290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>-0.030575</td>\n",
       "      <td>0.071396</td>\n",
       "      <td>0.085975</td>\n",
       "      <td>-0.094965</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>-0.230809</td>\n",
       "      <td>0.297596</td>\n",
       "      <td>0.284864</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>-0.108274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421334</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>0.259359</td>\n",
       "      <td>-0.111183</td>\n",
       "      <td>0.146449</td>\n",
       "      <td>0.164449</td>\n",
       "      <td>0.301710</td>\n",
       "      <td>-0.153121</td>\n",
       "      <td>0.089725</td>\n",
       "      <td>0.024233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>-0.099716</td>\n",
       "      <td>0.155442</td>\n",
       "      <td>0.045003</td>\n",
       "      <td>0.054721</td>\n",
       "      <td>0.055121</td>\n",
       "      <td>-0.204208</td>\n",
       "      <td>0.335802</td>\n",
       "      <td>0.325876</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>-0.106860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353255</td>\n",
       "      <td>-0.073040</td>\n",
       "      <td>0.146084</td>\n",
       "      <td>-0.139748</td>\n",
       "      <td>0.175605</td>\n",
       "      <td>0.189560</td>\n",
       "      <td>0.096747</td>\n",
       "      <td>-0.423187</td>\n",
       "      <td>0.084722</td>\n",
       "      <td>0.026079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3473</th>\n",
       "      <td>-0.182390</td>\n",
       "      <td>0.117490</td>\n",
       "      <td>0.040536</td>\n",
       "      <td>0.028133</td>\n",
       "      <td>0.118737</td>\n",
       "      <td>-0.290774</td>\n",
       "      <td>0.430598</td>\n",
       "      <td>0.375839</td>\n",
       "      <td>-0.037987</td>\n",
       "      <td>-0.235611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438862</td>\n",
       "      <td>-0.150765</td>\n",
       "      <td>0.165542</td>\n",
       "      <td>-0.209222</td>\n",
       "      <td>0.242864</td>\n",
       "      <td>0.189597</td>\n",
       "      <td>0.071091</td>\n",
       "      <td>-0.648700</td>\n",
       "      <td>-0.017442</td>\n",
       "      <td>0.066832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474</th>\n",
       "      <td>-0.020864</td>\n",
       "      <td>0.048926</td>\n",
       "      <td>0.082382</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>-0.050667</td>\n",
       "      <td>0.117091</td>\n",
       "      <td>0.134180</td>\n",
       "      <td>-0.021873</td>\n",
       "      <td>-0.053241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124748</td>\n",
       "      <td>-0.016818</td>\n",
       "      <td>0.079696</td>\n",
       "      <td>-0.039821</td>\n",
       "      <td>0.083431</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>0.077340</td>\n",
       "      <td>-0.120353</td>\n",
       "      <td>0.033854</td>\n",
       "      <td>0.064989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3475 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.004590  0.031992  0.149353 -0.103647  0.127495 -0.345164  0.255643   \n",
       "1     0.105189  0.132897  0.044019 -0.042159  0.179388 -0.327574  0.260852   \n",
       "2     0.131420  0.056091  0.078574 -0.137063  0.160461 -0.292200  0.185550   \n",
       "3     0.133949  0.110150  0.059820 -0.149727  0.158738 -0.300730  0.271797   \n",
       "4     0.089234  0.104596  0.083269  0.031202  0.128677 -0.220117  0.220752   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3470 -0.059292  0.093661 -0.053745 -0.151095  0.096664 -0.395085  0.341605   \n",
       "3471 -0.030575  0.071396  0.085975 -0.094965  0.092630 -0.230809  0.297596   \n",
       "3472 -0.099716  0.155442  0.045003  0.054721  0.055121 -0.204208  0.335802   \n",
       "3473 -0.182390  0.117490  0.040536  0.028133  0.118737 -0.290774  0.430598   \n",
       "3474 -0.020864  0.048926  0.082382  0.002714  0.003947 -0.050667  0.117091   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "0     0.259811  0.157978 -0.124985  ...  0.450639  0.028335  0.206324   \n",
       "1     0.266546  0.189100 -0.069553  ...  0.422972  0.107587  0.161347   \n",
       "2     0.263968  0.205161 -0.011723  ...  0.424150  0.119437  0.203855   \n",
       "3     0.353809  0.208494 -0.062116  ...  0.501836  0.067628  0.204282   \n",
       "4     0.196400  0.252232 -0.092232  ...  0.421696  0.113761  0.252042   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3470  0.403616  0.341046 -0.082672  ...  0.528257  0.013550  0.255730   \n",
       "3471  0.284864  0.196800 -0.108274  ...  0.421334  0.011176  0.259359   \n",
       "3472  0.325876  0.011340 -0.106860  ...  0.353255 -0.073040  0.146084   \n",
       "3473  0.375839 -0.037987 -0.235611  ...  0.438862 -0.150765  0.165542   \n",
       "3474  0.134180 -0.021873 -0.053241  ...  0.124748 -0.016818  0.079696   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "0    -0.063071  0.110578  0.085154  0.278110 -0.201369  0.117699  0.060710  \n",
       "1    -0.126538  0.073219  0.122300  0.380425  0.048626  0.157927 -0.044815  \n",
       "2    -0.044585  0.162057  0.188318  0.476668  0.031398  0.156915  0.177543  \n",
       "3    -0.040796  0.039807  0.139566  0.335987 -0.004681  0.147508  0.228402  \n",
       "4    -0.097961  0.155823  0.296137  0.377603 -0.072417  0.211872  0.034544  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3470 -0.059689  0.189930  0.266602  0.204304 -0.140718  0.181028  0.086290  \n",
       "3471 -0.111183  0.146449  0.164449  0.301710 -0.153121  0.089725  0.024233  \n",
       "3472 -0.139748  0.175605  0.189560  0.096747 -0.423187  0.084722  0.026079  \n",
       "3473 -0.209222  0.242864  0.189597  0.071091 -0.648700 -0.017442  0.066832  \n",
       "3474 -0.039821  0.083431 -0.000258  0.077340 -0.120353  0.033854  0.064989  \n",
       "\n",
       "[3475 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(document_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a78fe8",
   "metadata": {},
   "source": [
    "## Similarity measure\n",
    "\n",
    "Here cosine_similarity is used to compare the query vector and and the document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97a0df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a3d47",
   "metadata": {},
   "source": [
    "## Saving document vectors for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04a51f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = root / \"Pickled_files\" / \"Document_vectors\"\n",
    "dbfile = open(my_path, 'wb')\n",
    "pickle.dump(document_vecs, dbfile) \n",
    "dbfile.close()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Document_vectors\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "document_vecs = pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576c5e7",
   "metadata": {},
   "source": [
    "## Processing the query\n",
    "\n",
    "This code calculates the similarity scores between a given query and a set of documents. It first generates a document vector for each document in the set using the text_embedding function. Then, it generates a query vector using the same function. The cosine similarity between the query vector and each document vector is calculated and stored in the similarity_scores list. Finally, the documents are ranked in descending order based on their similarity scores with the query, and the top five documents are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4bce17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResults(query,ranked_docs):\n",
    "    print(query)\n",
    "    print()\n",
    "    for i in range(0,min(3,len(ranked_docs))):\n",
    "        print(ranked_docs[i][1])\n",
    "        print(ranked_docs[i][0])\n",
    "        print(\"---------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e2bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical reports\n",
      "\n",
      ".\\Docs\\Auto\\AU127-1.docx\n",
      "proof of claim ; medical reports as soon as possible , any person making claim must give us written proof of claim.the injured person may be required to take medical examinations by physicians we choose , as often as we reasonably require.we must be given authorization to obtain medical reports and copies of records pertinent to the claim . \n",
      "---------------------------------------------------------------------------------------------------------------\n",
      ".\\Docs\\Property\\eSols Property Owners Commercial Policy Wording 2018 Policies which incepted before 01 July.docx\n",
      "we will only invoke this right in exceptional circumstances as a result of you behaving inappropriately , for example : where we have a reasonable suspicion of fraud you use threatening or abusive behaviour or language or intimidation or bullying of our staff or suppliers where it is found that you , deliberately or recklessly , disclosed false information or failed to disclose important information claims you must report claims as soon as reasonably possible within 45 days of the insured event , by completing and submitting the claim form with all relevant information . \n",
      "---------------------------------------------------------------------------------------------------------------\n",
      ".\\Docs\\Property\\rsa_property_owners_policy_wording.docx\n",
      "thank you for your feedback we value your feedback and at the heart of our brand we remain dedicated to treating our customers as individuals and giving them the best possible service at all times.if we have fallen short of this promise , we apologise and aim to do everything possible to put things right.how we use your information please read the following carefully as it contains important information relating to the details that you have given us . \n",
      "---------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example usage: compare a query with a set of documents\n",
    "input_file = open(\"phrase_input.txt\",\"r\")\n",
    "query = input_file.readline()\n",
    "query_vec = text_embedding(query,skipgram_model)\n",
    "# Calculate similarity scores between the query and all documents\n",
    "similarity_scores = [cosine_similarity(query_vec, document_vec) for document_vec in document_vecs]\n",
    "# Rank documents based on similarity scores\n",
    "ranked_documents = [document for _, document in sorted(zip(similarity_scores, documents), reverse=True)]\n",
    "# Print the ranked documents\n",
    "printResults(query,ranked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90873a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1fd8c99ec767a3093790aadbd23b282cb2563d0e033731624352a553ec955a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
