{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99b9875",
   "metadata": {},
   "source": [
    "# Ranking documents using skip grams word embedding model for phrase queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d31553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import *\n",
    "from nltk.stem.porter import *\n",
    "import pickle\n",
    "import gensim \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338ec254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Uncomment in first run \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836cd19",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### The corpus/documents are extracted from the pickle files.\n",
    "#### Stemmer has been initialised to convert term to its root form (Example : received -> receive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923b920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "root = Path(\"../\")\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Documents\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "documents = pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52228594",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Skip-gram model is a type of neural network architecture that is commonly used for word embedding. It is a form of unsupervised learning that aims to learn the distributional representation of words.The basic idea behind skip-gram model is to predict the context words surrounding a given target word, rather than predicting the target word from the context words. The context words are defined as the words that occur within a certain window of the target word.\n",
    "\n",
    "The skip-gram model consists of an input layer, a hidden layer, and an output layer. The input layer represents the target word, and the output layer represents the context words. The hidden layer is used to transform the input word into a distributed representation, or embedding, which is used to predict the context words.During training, the skip-gram model is fed with a large corpus of text. For each target word in the corpus, a training example is created by randomly selecting one of its context words as the output, and setting the remaining context words as inputs to the model. The model is then trained to predict the output context word given the input context words.\n",
    "\n",
    "The training process involves adjusting the weights of the model to minimize the difference between the predicted context word and the actual context word. The weights are adjusted using backpropagation algorithm, which updates the weights based on the difference between the predicted and actual output.After training, the skip-gram model generates a dense vector representation for each word in the vocabulary. The vector representation captures the semantic and syntactic information of the word, and can be used as input to other machine learning models for various natural language processing tasks such as text classification, information retrieval, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81911a93",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "The following code initializes an empty list called data, which will be used to store the tokenized and preprocessed documents. Then, it loops over each document in the documents list and tokenizes each sentence into a list of words. Each word is then converted to lowercase and added to a temporary list called temp. The temp list is then appended to the data list, which now contains a list of tokenized and preprocessed documents.\n",
    "\n",
    "Next, the Word2Vec model from the gensim library is used to train a word embedding model on the data list. The min_count parameter specifies the minimum frequency count of a word in the corpus for it to be included in the model. The vector_size parameter specifies the dimensionality of the word vectors. The window parameter specifies the maximum distance between the target word and the context word within a sentence. The sg parameter specifies the training algorithm to be used - 1 for skip-gram and 0 for CBOW. In this case, sg is set to 1 which corresponds to the skip-gram algorithm for training.\n",
    "\n",
    "After training, the word embedding model skipgram_model contains dense vector representations for each word in the corpus. These vector representations can be used for various natural language processing tasks such as text classification, information retrieval, and machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9757e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "for i in documents: \n",
    "    temp = list()\n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i[0]): \n",
    "        temp.append(j.lower()) \n",
    "    data.append(temp)\n",
    "\n",
    "skipgram_model = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5,sg = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad520f34",
   "metadata": {},
   "source": [
    "## Generating word embeddings for documents\n",
    "\n",
    "The following code defines two functions.\n",
    "\n",
    "1.**preprocess(text)** takes a string of text as input, tokenizes it, removes stop words and filters out words with length less than two. It returns a list of preprocessed tokens.\n",
    "\n",
    "2.**text_embedding(text, model)** takes a string of text and a pre-trained word embedding model as inputs. It preprocesses the text using the preprocess() function, obtains the word embeddings for each word in the preprocessed text from the pre-trained word embedding model, takes the average of the word embeddings to generate a document embedding, and returns the document embedding as a numpy array.\n",
    "\n",
    "Finally, the **document_vecs** variable is assigned a list of document embeddings generated using the text_embedding() function and the pre-trained word embedding model (skipgram_model) for each document in the documents list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed556e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = []\n",
    "    for word in word_tokenize(text, language='english'):\n",
    "        if len(word) >= 2 and word not in stop_words:\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# Define a function to generate text embeddings\n",
    "def text_embedding(text, model):\n",
    "    # Preprocess the text (tokenize, remove stop words, stem)\n",
    "    preprocessed_text = preprocess(text)\n",
    "    # Get the word embeddings for each word in the document\n",
    "    word_embeddings = [model.wv[word] for word in preprocessed_text if word in model.wv.key_to_index]\n",
    "    # Take the average of the word embeddings to generate a document embedding\n",
    "    if len(word_embeddings) > 0:\n",
    "        document_embedding = np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        document_embedding = np.zeros(model.vector_size)\n",
    "    return document_embedding\n",
    "\n",
    "document_vecs = [text_embedding(document[0], skipgram_model) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275971a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.029961</td>\n",
       "      <td>0.137852</td>\n",
       "      <td>0.062892</td>\n",
       "      <td>-0.068636</td>\n",
       "      <td>0.146065</td>\n",
       "      <td>-0.310918</td>\n",
       "      <td>0.260103</td>\n",
       "      <td>0.312034</td>\n",
       "      <td>0.085926</td>\n",
       "      <td>-0.058731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274211</td>\n",
       "      <td>-0.067892</td>\n",
       "      <td>0.193503</td>\n",
       "      <td>-0.108390</td>\n",
       "      <td>0.133346</td>\n",
       "      <td>0.016425</td>\n",
       "      <td>0.352965</td>\n",
       "      <td>-0.080150</td>\n",
       "      <td>0.173055</td>\n",
       "      <td>0.033377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.048956</td>\n",
       "      <td>0.234289</td>\n",
       "      <td>0.091454</td>\n",
       "      <td>-0.085685</td>\n",
       "      <td>0.082340</td>\n",
       "      <td>-0.287559</td>\n",
       "      <td>0.304864</td>\n",
       "      <td>0.296115</td>\n",
       "      <td>0.054177</td>\n",
       "      <td>0.057240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189385</td>\n",
       "      <td>0.059318</td>\n",
       "      <td>0.233837</td>\n",
       "      <td>-0.185014</td>\n",
       "      <td>0.125110</td>\n",
       "      <td>0.014443</td>\n",
       "      <td>0.592293</td>\n",
       "      <td>0.182993</td>\n",
       "      <td>0.217441</td>\n",
       "      <td>-0.070333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016117</td>\n",
       "      <td>0.101110</td>\n",
       "      <td>0.106387</td>\n",
       "      <td>-0.169617</td>\n",
       "      <td>0.151749</td>\n",
       "      <td>-0.275378</td>\n",
       "      <td>0.192407</td>\n",
       "      <td>0.284254</td>\n",
       "      <td>0.063564</td>\n",
       "      <td>0.102821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243920</td>\n",
       "      <td>0.053439</td>\n",
       "      <td>0.261050</td>\n",
       "      <td>-0.167039</td>\n",
       "      <td>0.216173</td>\n",
       "      <td>0.064337</td>\n",
       "      <td>0.637676</td>\n",
       "      <td>0.147637</td>\n",
       "      <td>0.076408</td>\n",
       "      <td>0.102397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003193</td>\n",
       "      <td>0.060116</td>\n",
       "      <td>0.077803</td>\n",
       "      <td>-0.178501</td>\n",
       "      <td>0.169118</td>\n",
       "      <td>-0.322478</td>\n",
       "      <td>0.214613</td>\n",
       "      <td>0.379391</td>\n",
       "      <td>0.065270</td>\n",
       "      <td>0.057274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311613</td>\n",
       "      <td>-0.011907</td>\n",
       "      <td>0.250686</td>\n",
       "      <td>-0.105574</td>\n",
       "      <td>0.113224</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.529442</td>\n",
       "      <td>0.129211</td>\n",
       "      <td>0.103091</td>\n",
       "      <td>0.152931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064057</td>\n",
       "      <td>0.228277</td>\n",
       "      <td>0.115788</td>\n",
       "      <td>-0.094056</td>\n",
       "      <td>0.122794</td>\n",
       "      <td>-0.212706</td>\n",
       "      <td>0.220429</td>\n",
       "      <td>0.212295</td>\n",
       "      <td>0.121120</td>\n",
       "      <td>0.042652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221088</td>\n",
       "      <td>0.066263</td>\n",
       "      <td>0.284462</td>\n",
       "      <td>-0.167477</td>\n",
       "      <td>0.198747</td>\n",
       "      <td>0.168184</td>\n",
       "      <td>0.530396</td>\n",
       "      <td>0.123433</td>\n",
       "      <td>0.212684</td>\n",
       "      <td>0.025017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>-0.118166</td>\n",
       "      <td>0.080370</td>\n",
       "      <td>-0.036874</td>\n",
       "      <td>-0.134574</td>\n",
       "      <td>0.177163</td>\n",
       "      <td>-0.342189</td>\n",
       "      <td>0.307943</td>\n",
       "      <td>0.371783</td>\n",
       "      <td>0.085643</td>\n",
       "      <td>-0.011377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435848</td>\n",
       "      <td>-0.052106</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>-0.166312</td>\n",
       "      <td>0.224200</td>\n",
       "      <td>0.195284</td>\n",
       "      <td>0.424065</td>\n",
       "      <td>-0.084877</td>\n",
       "      <td>0.113451</td>\n",
       "      <td>0.105040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>-0.051591</td>\n",
       "      <td>0.098346</td>\n",
       "      <td>0.058517</td>\n",
       "      <td>-0.072703</td>\n",
       "      <td>0.126266</td>\n",
       "      <td>-0.244646</td>\n",
       "      <td>0.276554</td>\n",
       "      <td>0.280397</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>0.010057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273462</td>\n",
       "      <td>-0.015334</td>\n",
       "      <td>0.217563</td>\n",
       "      <td>-0.182987</td>\n",
       "      <td>0.229434</td>\n",
       "      <td>0.126491</td>\n",
       "      <td>0.399488</td>\n",
       "      <td>-0.086635</td>\n",
       "      <td>0.098177</td>\n",
       "      <td>0.005683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>-0.122357</td>\n",
       "      <td>0.128365</td>\n",
       "      <td>0.046634</td>\n",
       "      <td>0.126418</td>\n",
       "      <td>0.052945</td>\n",
       "      <td>-0.296985</td>\n",
       "      <td>0.271161</td>\n",
       "      <td>0.230271</td>\n",
       "      <td>0.071167</td>\n",
       "      <td>-0.098535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283309</td>\n",
       "      <td>-0.030030</td>\n",
       "      <td>0.039129</td>\n",
       "      <td>-0.072816</td>\n",
       "      <td>0.084887</td>\n",
       "      <td>0.247428</td>\n",
       "      <td>0.109177</td>\n",
       "      <td>-0.357931</td>\n",
       "      <td>0.095543</td>\n",
       "      <td>0.124211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3473</th>\n",
       "      <td>-0.141574</td>\n",
       "      <td>0.083228</td>\n",
       "      <td>0.023239</td>\n",
       "      <td>0.177175</td>\n",
       "      <td>0.036909</td>\n",
       "      <td>-0.447578</td>\n",
       "      <td>0.315401</td>\n",
       "      <td>0.190893</td>\n",
       "      <td>0.052432</td>\n",
       "      <td>-0.145392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330529</td>\n",
       "      <td>-0.066754</td>\n",
       "      <td>-0.023257</td>\n",
       "      <td>-0.066352</td>\n",
       "      <td>0.081898</td>\n",
       "      <td>0.274095</td>\n",
       "      <td>0.035102</td>\n",
       "      <td>-0.518561</td>\n",
       "      <td>0.058561</td>\n",
       "      <td>0.131167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474</th>\n",
       "      <td>-0.051640</td>\n",
       "      <td>0.050516</td>\n",
       "      <td>0.087241</td>\n",
       "      <td>0.034654</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>-0.082693</td>\n",
       "      <td>0.119871</td>\n",
       "      <td>0.124260</td>\n",
       "      <td>-0.014044</td>\n",
       "      <td>-0.046776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081039</td>\n",
       "      <td>-0.012385</td>\n",
       "      <td>0.043313</td>\n",
       "      <td>-0.015758</td>\n",
       "      <td>0.088682</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>0.080367</td>\n",
       "      <td>-0.093672</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.065842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3475 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.029961  0.137852  0.062892 -0.068636  0.146065 -0.310918  0.260103   \n",
       "1     0.048956  0.234289  0.091454 -0.085685  0.082340 -0.287559  0.304864   \n",
       "2     0.016117  0.101110  0.106387 -0.169617  0.151749 -0.275378  0.192407   \n",
       "3    -0.003193  0.060116  0.077803 -0.178501  0.169118 -0.322478  0.214613   \n",
       "4     0.064057  0.228277  0.115788 -0.094056  0.122794 -0.212706  0.220429   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3470 -0.118166  0.080370 -0.036874 -0.134574  0.177163 -0.342189  0.307943   \n",
       "3471 -0.051591  0.098346  0.058517 -0.072703  0.126266 -0.244646  0.276554   \n",
       "3472 -0.122357  0.128365  0.046634  0.126418  0.052945 -0.296985  0.271161   \n",
       "3473 -0.141574  0.083228  0.023239  0.177175  0.036909 -0.447578  0.315401   \n",
       "3474 -0.051640  0.050516  0.087241  0.034654  0.006887 -0.082693  0.119871   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "0     0.312034  0.085926 -0.058731  ...  0.274211 -0.067892  0.193503   \n",
       "1     0.296115  0.054177  0.057240  ...  0.189385  0.059318  0.233837   \n",
       "2     0.284254  0.063564  0.102821  ...  0.243920  0.053439  0.261050   \n",
       "3     0.379391  0.065270  0.057274  ...  0.311613 -0.011907  0.250686   \n",
       "4     0.212295  0.121120  0.042652  ...  0.221088  0.066263  0.284462   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3470  0.371783  0.085643 -0.011377  ...  0.435848 -0.052106  0.202213   \n",
       "3471  0.280397  0.028105  0.010057  ...  0.273462 -0.015334  0.217563   \n",
       "3472  0.230271  0.071167 -0.098535  ...  0.283309 -0.030030  0.039129   \n",
       "3473  0.190893  0.052432 -0.145392  ...  0.330529 -0.066754 -0.023257   \n",
       "3474  0.124260 -0.014044 -0.046776  ...  0.081039 -0.012385  0.043313   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "0    -0.108390  0.133346  0.016425  0.352965 -0.080150  0.173055  0.033377  \n",
       "1    -0.185014  0.125110  0.014443  0.592293  0.182993  0.217441 -0.070333  \n",
       "2    -0.167039  0.216173  0.064337  0.637676  0.147637  0.076408  0.102397  \n",
       "3    -0.105574  0.113224  0.001763  0.529442  0.129211  0.103091  0.152931  \n",
       "4    -0.167477  0.198747  0.168184  0.530396  0.123433  0.212684  0.025017  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3470 -0.166312  0.224200  0.195284  0.424065 -0.084877  0.113451  0.105040  \n",
       "3471 -0.182987  0.229434  0.126491  0.399488 -0.086635  0.098177  0.005683  \n",
       "3472 -0.072816  0.084887  0.247428  0.109177 -0.357931  0.095543  0.124211  \n",
       "3473 -0.066352  0.081898  0.274095  0.035102 -0.518561  0.058561  0.131167  \n",
       "3474 -0.015758  0.088682  0.020469  0.080367 -0.093672  0.015061  0.065842  \n",
       "\n",
       "[3475 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(document_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a78fe8",
   "metadata": {},
   "source": [
    "## Similarity measure\n",
    "\n",
    "Here cosine_similarity is used to compare the query vector and and the document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a0df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a3d47",
   "metadata": {},
   "source": [
    "## Saving document vectors for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04a51f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = root / \"Pickled_files\" / \"Document_vectors\"\n",
    "dbfile = open(my_path, 'wb')\n",
    "pickle.dump(document_vecs, dbfile) \n",
    "dbfile.close()\n",
    "\n",
    "my_path = root / \"Pickled_files\" / \"Document_vectors\"\n",
    "dbfile = open(my_path, 'rb')     \n",
    "document_vecs = pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576c5e7",
   "metadata": {},
   "source": [
    "## Processing the query\n",
    "\n",
    "This code calculates the similarity scores between a given query and a set of documents. It first generates a document vector for each document in the set using the text_embedding function. Then, it generates a query vector using the same function. The cosine similarity between the query vector and each document vector is calculated and stored in the similarity_scores list. Finally, the documents are ranked in descending order based on their similarity scores with the query, and the top five documents are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e2bf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('excluded drivers and driving without permission except for certain accident benefits coverage , there is no coverage ( including coverage for occupants ) under this policy if the automobile is used or operated by a person in possession of the automobile without the owner ’ s consent or is driven by a person named as an excluded driver of the automobile policy or a person who , at the time he or she willingly becomes an occupant of an automobile , knows or ought reasonably to know that the automobile is being used or operated by a person in possession of the automobile without the owner ’ s consent . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\1215E.2.docx'),\n",
       " ('we will not pay damages to or for any household member who has a massachusetts auto policy of his or her own or who is covered by a massachusetts auto policy of another household member providing underinsured auto insurance with higher limits.anyone else while occupying your auto.we will not pay damages to or for anyone else who has a massachusetts auto policy of his or her own or who is covered by a massachusetts auto policy of another household member providing underinsured auto coverage . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\7thEditionPolicy.docx'),\n",
       " ('this exception does not apply to any do- mestic employee who is not entitled to any workers ’ compensation benefits.anyone injured while occupying an auto without a reasonable belief that he or she had the consent of the owner to do so.a household member , other than your spouse , while occupying or struck by an auto owned or regularly used by you or any household member unless a premium for this part is shown for that auto on the coverage selections page . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\7thEditionPolicy.docx'),\n",
       " ('automobiles , other than a described automobile , are covered as described in this subsection when rented by you , or by your spouse who lives with you , for periods of not more than 30 days , but only with respect to the liability of the person renting the automobile arising from the negligence of the driver of that automobile , and only if the driver is not an excluded driver under this policy . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\1215E.2.docx'),\n",
       " ('anyone else while using with your per- mission a covered auto you own , hire or borrow except : the owner or anyone else from whom you hire or borrow a covered auto.this exception does not apply if the covered auto is a trailer con- nected to a covered auto you own.your employee if the covered auto is owned by that employee or a member of his or her household . ',\n",
       "  '.\\\\Docs\\\\Auto\\\\Business-Auto-Policy-CA0001-03-10.docx')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage: compare a query with a set of documents\n",
    "query = \"Excluded driver\"\n",
    "\n",
    "query_vec = text_embedding(query,skipgram_model)\n",
    "# Calculate similarity scores between the query and all documents\n",
    "similarity_scores = [cosine_similarity(query_vec, document_vec) for document_vec in document_vecs]\n",
    "# Rank documents based on similarity scores\n",
    "ranked_documents = [document for _, document in sorted(zip(similarity_scores, documents), reverse=True)]\n",
    "# Print the ranked documents\n",
    "ranked_documents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90873a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1fd8c99ec767a3093790aadbd23b282cb2563d0e033731624352a553ec955a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
